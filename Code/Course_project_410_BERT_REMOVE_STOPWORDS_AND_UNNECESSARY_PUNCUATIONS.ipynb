{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"Course_project_410_BERT_REMOVE_STOPWORDS_AND_UNNECESSARY_PUNCUATIONS.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VtwexhukQ5Cn","outputId":"6f9bc168-5522-4595-bd0d-bc987f2e0b03"},"source":["!pip install jsonlines"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: jsonlines in /usr/local/lib/python3.6/dist-packages (1.2.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rAhreXcYUBTm","outputId":"3d2b70b1-2c4a-45f9-e642-b75e2314dee1"},"source":["#create readable train data csv\n","\n","import os\n","import jsonlines\n","import csv\n","import pandas as pd\n","import random\n","import nltk\n","from nltk.corpus import stopwords\n","\n","\n","nltk.download('stopwords')\n","\n","data = []\n","\n","with open(\"train.jsonl\",\"r+\",encoding='utf-8') as f:\n","    for item in jsonlines.Reader(f):\n","        if item['label'] == 'SARCASM':\n","            item['label'] = 0\n","        else:\n","            item['label'] = 1\n","\n","        data.append(item)\n","\n","with open('twitter.csv', 'w+', newline='', encoding='utf-8') as f:\n","    csv_writer = csv.writer(f)\n","    csv_head = ['context', 'response', 'label']\n","    csv_writer.writerow(csv_head)\n","\n","    for i in range(len(data)):\n","      data[i]['context'] =' '.join([word for word in str(data[i]['context']).split() if not word in stopwords.words('english')])\n","      data[i]['response'] =' '.join([word for word in str(data[i]['response']).split() if not word in stopwords.words('english')])\n"," \n","    for i in range(len(data)):\n","        data_row = [(data[i]['context']).replace('@USER', '').replace('<URL>', '').replace('[', '').replace(']', '').replace(\"'\",  '').replace('\"', '').replace('.', '').replace('#', ''),\n","                    (data[i]['response']).replace('@USER', '').replace('<URL>','').replace(\"'\",  '').replace('\"', '').replace('.', '').replace('#', '').replace('.', ''), data[i]['label']]\n","        csv_writer.writerow(data_row)\n","\n","total = pd.read_csv(\"twitter.csv\")\n","\n","index = set(range(total.shape[0]))\n","K_fold = []\n","for i in range(5):\n","    if i == 4:\n","        tmp = index\n","    else:\n","        tmp = random.sample(index, int(1.0 / 5 * total.shape[0]))\n","    index = index - set(tmp)\n","    print(\"Number:\", len(tmp))\n","    K_fold.append(tmp)\n","\n","for i in range(5):\n","    print(\"Fold\", i)\n","    os.system(\"mkdir data_{}\".format(i))\n","    dev_index = list(K_fold[i])\n","    train_index = []\n","    for j in range(5):\n","        if j != i:\n","            train_index += K_fold[j]\n","    total.iloc[train_index].to_csv(\"data_{}/train.csv\".format(i))\n","    total.iloc[dev_index].to_csv(\"data_{}/dev.csv\".format(i))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","Number: 1000\n","Number: 1000\n","Number: 1000\n","Number: 1000\n","Number: 1000\n","Fold 0\n","Fold 1\n","Fold 2\n","Fold 3\n","Fold 4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jag_Pj7tQklb","outputId":"5d3a013d-7b23-4bb7-dbdc-02a87e5b17e1"},"source":["#create readable test data csv\n","\n","data = []\n","\n","with open('test.jsonl',\"r+\",encoding='utf-8') as f:\n","    for item in jsonlines.Reader(f):\n","        data.append(item)\n","\n","with open('testdata.csv', 'w+', newline='', encoding='utf-8') as f:\n","    csv_writer = csv.writer(f)\n","    csv_head = ['context', 'response', 'id']\n","    csv_writer.writerow(csv_head)\n","\n","    for i in range(len(data)):\n","      data[i]['context'] =' '.join([word for word in str(data[i]['context']).split() if not word in stopwords.words('english')])\n","      data[i]['response'] =' '.join([word for word in str(data[i]['response']).split() if not word in stopwords.words('english')])\n","    \n","    for i in range(len(data)):\n","        data_row = [(data[i]['context']).replace('@USER', '').replace('<URL>', '').replace('[', '').replace(']', '').replace(\"'\",  '').replace('\"', '').replace('.', '').replace('#', ''),\n","                    (data[i]['response']).replace('@USER', '').replace('<URL>','').replace(\"'\",  '').replace('\"', '').replace('.', '').replace('#', '').replace('.', ''), data[i]['id']]\n","        csv_writer.writerow(data_row)\n","\n","total = pd.read_csv(\"testdata.csv\")\n","\n","index = set(range(total.shape[0]))\n","K_fold = []\n","for i in range(5):\n","    if i == 4:\n","        tmp = index\n","    else:\n","        tmp = random.sample(index, int(1.0 / 5 * total.shape[0]))\n","    index = index - set(tmp)\n","    print(\"Number:\", len(tmp))\n","    K_fold.append(tmp)\n","\n","for i in range(5):\n","    print(\"Fold\", i)\n","    os.system(\"mkdir data_{}\".format(i))\n","    dev_index = list(K_fold[i])\n","    train_index = []\n","    for j in range(5):\n","        if j != i:\n","            train_index += K_fold[j]\n","    total.iloc[train_index].to_csv(\"data_{}/train.csv\".format(i))\n","    total.iloc[dev_index].to_csv(\"data_{}/dev.csv\".format(i))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number: 360\n","Number: 360\n","Number: 360\n","Number: 360\n","Number: 360\n","Fold 0\n","Fold 1\n","Fold 2\n","Fold 3\n","Fold 4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tZRSiDYNQLSr","outputId":"3ac4fec7-9600-4def-a3ab-fb1dd883c12f"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n","\r\u001b[K     |▎                               | 10kB 21.6MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 24.6MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 13.0MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 10.2MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61kB 5.9MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71kB 6.3MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 6.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 92kB 6.8MB/s eta 0:00:01\r\u001b[K     |██▌                             | 102kB 7.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 112kB 7.3MB/s eta 0:00:01\r\u001b[K     |███                             | 122kB 7.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 133kB 7.3MB/s eta 0:00:01\r\u001b[K     |███▌                            | 143kB 7.3MB/s eta 0:00:01\r\u001b[K     |███▉                            | 153kB 7.3MB/s eta 0:00:01\r\u001b[K     |████                            | 163kB 7.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 174kB 7.3MB/s eta 0:00:01\r\u001b[K     |████▌                           | 184kB 7.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 194kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 204kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 215kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 225kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 235kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 245kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 256kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 266kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 276kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 286kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 296kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 307kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 317kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 327kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 337kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 348kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 358kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 368kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 378kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 389kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 399kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 409kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 419kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 430kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 440kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 450kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 460kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 471kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 481kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 491kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 501kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 512kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 522kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 532kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 542kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 552kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 563kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 573kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 583kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 593kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 604kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 614kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 624kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 634kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 645kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 655kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 665kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 675kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 686kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 696kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 706kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 716kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 727kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 737kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 747kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 757kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 768kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 778kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 788kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 798kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 808kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 819kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 829kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 839kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 849kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 860kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 870kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 880kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 890kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 901kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 911kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 921kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 931kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 942kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 952kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 962kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 972kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 983kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 993kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0MB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.0MB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.0MB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.0MB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.0MB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1MB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.1MB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.1MB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1MB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.1MB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.1MB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1MB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.2MB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2MB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2MB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2MB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2MB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.3MB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3MB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3MB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 7.3MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Collecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 45.9MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 44.2MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Collecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 63.1MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=7ad3947f58deff7134f26f916e29d24e43d4f6e68a685f21405aad9967cffeda\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LwruXiTXsORZ"},"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","\n","# specify CPU/GPU\n","device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"87T5BetZQYQv","outputId":"17d4fd8f-baa4-43fd-91aa-a17868b480d5"},"source":["td = pd.read_csv(\"testdata.csv\")\n","td['text'] = td['response']+td['context']  \n","\n","df = pd.read_csv(\"twitter.csv\")\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>context</th>\n","      <th>response</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A minor child deserves privacy kept politics  ...</td>\n","      <td>I get  obviously care wouldve moved right a...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Why loser ? Hes Press Secretary,   make excu...</td>\n","      <td>trying protest  Talking labels label WTF mak...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Donald J  Trump guilty charged  The evidence c...</td>\n","      <td>He makes insane money MOVIES , Einstein ! L...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Jamie Raskin tanked Doug Collins  Collins look...</td>\n","      <td>Meanwhile Trump even release SAT scores Whar...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Man  ’ gone “ sides ” apocalypse one day  ,  T...</td>\n","      <td>Pretty Sure Anti-Lincoln Crowd Claimed That ...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             context  ... label\n","0  A minor child deserves privacy kept politics  ...  ...     0\n","1    Why loser ? Hes Press Secretary,   make excu...  ...     0\n","2  Donald J  Trump guilty charged  The evidence c...  ...     0\n","3  Jamie Raskin tanked Doug Collins  Collins look...  ...     0\n","4  Man  ’ gone “ sides ” apocalypse one day  ,  T...  ...     0\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YOckP_sYQcda","outputId":"320d957b-b888-4c24-995b-c10a89774aae"},"source":["df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5000, 3)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_F9FfyFSQl_5","outputId":"3778b34a-2228-4b9d-b67f-0f6041467cc5"},"source":["# check class distribution\n","df['label'].value_counts(normalize = True)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    0.5\n","0    0.5\n","Name: label, dtype: float64"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289},"id":"hPVu_ZPTU3Q0","outputId":"e9810f22-64c4-42d0-b284-695e53ef8e17"},"source":["td.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>context</th>\n","      <th>response</th>\n","      <th>id</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Well ’ problematic AF ,   My 5 year old  asked...</td>\n","      <td>My 3 year old , finished reading Nietzsche ...</td>\n","      <td>twitter_1</td>\n","      <td>My 3 year old , finished reading Nietzsche ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Last week Fake News said section powerful , co...</td>\n","      <td>How many verifiable lies told ? 15,000+ docu...</td>\n","      <td>twitter_2</td>\n","      <td>How many verifiable lies told ? 15,000+ docu...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Let ’ Aplaud Brett When deserves coached amaz...</td>\n","      <td>Maybe Docs scrub coach  I mean get hammered...</td>\n","      <td>twitter_3</td>\n","      <td>Maybe Docs scrub coach  I mean get hammered...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Women generally hate president  Whats men ?,  ...</td>\n","      <td>cover real hate inside   The left nutshell !</td>\n","      <td>twitter_4</td>\n","      <td>cover real hate inside   The left nutshell !...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Dear media Remoaners , excitedly sharing clips...</td>\n","      <td>The irony even ask</td>\n","      <td>twitter_5</td>\n","      <td>The irony even ask Dear media Remoaners , e...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             context  ...                                               text\n","0  Well ’ problematic AF ,   My 5 year old  asked...  ...     My 3 year old , finished reading Nietzsche ...\n","1  Last week Fake News said section powerful , co...  ...    How many verifiable lies told ? 15,000+ docu...\n","2   Let ’ Aplaud Brett When deserves coached amaz...  ...     Maybe Docs scrub coach  I mean get hammered...\n","3  Women generally hate president  Whats men ?,  ...  ...    cover real hate inside   The left nutshell !...\n","4  Dear media Remoaners , excitedly sharing clips...  ...     The irony even ask Dear media Remoaners , e...\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289},"id":"xalsFDNyQp66","outputId":"95f8efeb-ca42-46c0-e307-7a8a23c35829"},"source":["#May try three kinds of situations: context alone, reponse alone and context +response to see which one performs the best, here I just combine the response and context\n","df['text'] = df['context']+df['response']\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>context</th>\n","      <th>response</th>\n","      <th>label</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A minor child deserves privacy kept politics  ...</td>\n","      <td>I get  obviously care wouldve moved right a...</td>\n","      <td>0</td>\n","      <td>A minor child deserves privacy kept politics  ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Why loser ? Hes Press Secretary,   make excu...</td>\n","      <td>trying protest  Talking labels label WTF mak...</td>\n","      <td>0</td>\n","      <td>Why loser ? Hes Press Secretary,   make excu...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Donald J  Trump guilty charged  The evidence c...</td>\n","      <td>He makes insane money MOVIES , Einstein ! L...</td>\n","      <td>0</td>\n","      <td>Donald J  Trump guilty charged  The evidence c...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Jamie Raskin tanked Doug Collins  Collins look...</td>\n","      <td>Meanwhile Trump even release SAT scores Whar...</td>\n","      <td>0</td>\n","      <td>Jamie Raskin tanked Doug Collins  Collins look...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Man  ’ gone “ sides ” apocalypse one day  ,  T...</td>\n","      <td>Pretty Sure Anti-Lincoln Crowd Claimed That ...</td>\n","      <td>0</td>\n","      <td>Man  ’ gone “ sides ” apocalypse one day  ,  T...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             context  ...                                               text\n","0  A minor child deserves privacy kept politics  ...  ...  A minor child deserves privacy kept politics  ...\n","1    Why loser ? Hes Press Secretary,   make excu...  ...    Why loser ? Hes Press Secretary,   make excu...\n","2  Donald J  Trump guilty charged  The evidence c...  ...  Donald J  Trump guilty charged  The evidence c...\n","3  Jamie Raskin tanked Doug Collins  Collins look...  ...  Jamie Raskin tanked Doug Collins  Collins look...\n","4  Man  ’ gone “ sides ” apocalypse one day  ,  T...  ...  Man  ’ gone “ sides ” apocalypse one day  ,  T...\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"XryjXIYhQ_gM"},"source":["train_text, val_text, train_labels, val_labels = train_test_split(df['text'], df['label'], \n","                                                                    random_state=2018, \n","                                                                    test_size=0.3, \n","                                                                    stratify=df['label'])\n","\n","test_text = td['text']    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VG9h0KEotC_r"},"source":["# import BERT-base pretrained model\n","bert = AutoModel.from_pretrained('bert-base-uncased')\n","\n","# Load the BERT tokenizer\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qPnTpMt8tJLp"},"source":["# sample data\n","text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n","\n","# encode text\n","sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MmGxHveVtJ4f","outputId":"649ddf7c-58a9-4f92-c381-4e69a4b2292a"},"source":["print(sent_id)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":285},"id":"TGYSbQzUtLwS","outputId":"13919b79-6af7-4dda-f43c-0c936a68f558"},"source":["# get length of all the messages in the train set\n","seq_len = [len(i.split()) for i in train_text]\n","\n","pd.Series(seq_len).hist(bins = 30)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f837a9866d8>"]},"metadata":{"tags":[]},"execution_count":15},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"OMzKcnYxtOFP"},"source":["#could change to get best answer, maybe 400 better? could try \n","max_seq_len = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g6mRLN3vtQXX","outputId":"57d3953b-0cc4-4170-f810-1ea4a7cad0f0"},"source":["# tokenize and encode sequences in the training set\n","tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# tokenize and encode sequences in the validation set\n","tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# tokenize and encode sequences in the test set\n","tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"kD_IfCzotVxj"},"source":["# for train set\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","# for validation set\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","# for test set\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","#test_y = torch.tensor(test_labels.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OaDhHt7NtZRZ"},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#define a batch size \n","batch_size = 28\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oc380qoYtfE5"},"source":["# freeze all the parameters\n","for param in bert.parameters():\n","    param.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y0nqk53itiDN"},"source":["class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert):\n","      \n","      super(BERT_Arch, self).__init__()\n","\n","      self.bert = bert \n","      \n","      # dropout layer\n","      self.dropout = nn.Dropout(0.1)\n","      \n","      # relu activation function\n","      self.relu =  nn.ReLU()\n","\n","      # dense layer 1\n","      self.fc1 = nn.Linear(768,512)\n","      \n","      # dense layer 2 (Output layer)\n","      self.fc2 = nn.Linear(512,2)\n","\n","      #softmax activation function\n","      self.softmax = nn.LogSoftmax(dim=1)\n","\n","    #define the forward pass\n","    def forward(self, sent_id, mask):\n","\n","      #pass the inputs to the model  \n","      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n","      \n","      x = self.fc1(cls_hs)\n","\n","      x = self.relu(x)\n","\n","      x = self.dropout(x)\n","\n","      # output layer\n","      x = self.fc2(x)\n","      \n","      # apply softmax activation\n","      x = self.softmax(x)\n","\n","      return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pyVFwzk5tjXo"},"source":["# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(bert)\n","\n","# push the model to GPU\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IGHNgzsDtqfP"},"source":["# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr = 1e-3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5MHWVvVDttbi","outputId":"3652fd62-8a90-4cad-8af3-8ca503762019"},"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","#compute the class weights\n","class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n","\n","print(class_wts)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1. 1.]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ytxaBXCDtwk_"},"source":["# convert class weights to tensor\n","weights= torch.tensor(class_wts,dtype=torch.float)\n","weights = weights.to(device)\n","\n","# loss function\n","cross_entropy  = nn.NLLLoss(weight=weights) \n","\n","# number of training epochs\n","epochs = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlrAx2VLt0F5"},"source":["# function to train the model\n","def train():\n","  \n","  model.train()\n","\n","  total_loss, total_accuracy = 0, 0\n","  \n","  # empty list to save model predictions\n","  total_preds=[]\n","  \n","  # iterate over batches\n","  for step,batch in enumerate(train_dataloader):\n","    \n","    # progress update after every 50 batches.\n","    if step % 50 == 0 and not step == 0:\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n","\n","    # push the batch to gpu\n","    batch = [r.to(device) for r in batch]\n"," \n","    sent_id, mask, labels = batch\n","\n","    # clear previously calculated gradients \n","    model.zero_grad()        \n","\n","    # get model predictions for the current batch\n","    preds = model(sent_id, mask)\n","\n","    # compute the loss between actual and predicted values\n","    loss = cross_entropy(preds, labels)\n","\n","    # add on to the total loss\n","    total_loss = total_loss + loss.item()\n","\n","    # backward pass to calculate the gradients\n","    loss.backward()\n","\n","    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","    # update parameters\n","    optimizer.step()\n","\n","    # model predictions are stored on GPU. So, push it to CPU\n","    preds=preds.detach().cpu().numpy()\n","\n","    # append the model predictions\n","    total_preds.append(preds)\n","\n","  # compute the training loss of the epoch\n","  avg_loss = total_loss / len(train_dataloader)\n","  \n","  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","\n","  #returns the loss and predictions\n","  return avg_loss, total_preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RmW_LsLct4lQ"},"source":["# function for evaluating the model\n","def evaluate():\n","  \n","  print(\"\\nEvaluating...\")\n","  \n","  # deactivate dropout layers\n","  model.eval()\n","\n","  total_loss, total_accuracy = 0, 0\n","  \n","  # empty list to save the model predictions\n","  total_preds = []\n","\n","  # iterate over batches\n","  for step,batch in enumerate(val_dataloader):\n","    \n","    # Progress update every 50 batches.\n","    if step % 50 == 0 and not step == 0:\n","      \n","      # Calculate elapsed time in minutes.\n","      #elapsed = format_time(time.time() - t0)\n","            \n","      # Report progress.\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n","\n","    # push the batch to gpu\n","    batch = [t.to(device) for t in batch]\n","\n","    sent_id, mask, labels = batch\n","\n","    # deactivate autograd\n","    with torch.no_grad():\n","      \n","      # model predictions\n","      preds = model(sent_id, mask)\n","\n","      # compute the validation loss between actual and predicted values\n","      loss = cross_entropy(preds,labels)\n","\n","      total_loss = total_loss + loss.item()\n","\n","      preds = preds.detach().cpu().numpy()\n","\n","      total_preds.append(preds)\n","\n","  # compute the validation loss of the epoch\n","  avg_loss = total_loss / len(val_dataloader) \n","\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","\n","  return avg_loss, total_preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DqLI_oGJt58w","outputId":"0dfd55ec-a4eb-4457-9896-f43dac87f656"},"source":["# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","valid_losses=[]\n","\n","#for each epoch\n","for epoch in range(epochs):\n","     \n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","    \n","    #train model\n","    train_loss, _ = train()\n","    \n","    #evaluate model\n","    valid_loss, _ = evaluate()\n","    \n","    #save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","    \n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","    \n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n"," Epoch 1 / 10\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Evaluating...\n","  Batch    50  of     58.\n","\n","Training Loss: 0.671\n","Validation Loss: 0.652\n","\n"," Epoch 2 / 10\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Evaluating...\n","  Batch    50  of     58.\n","\n","Training Loss: 0.607\n","Validation Loss: 0.596\n","\n"," Epoch 3 / 10\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Evaluating...\n","  Batch    50  of     58.\n","\n","Training Loss: 0.594\n","Validation Loss: 0.585\n","\n"," Epoch 4 / 10\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Evaluating...\n","  Batch    50  of     58.\n","\n","Training Loss: 0.591\n","Validation Loss: 0.576\n","\n"," Epoch 5 / 10\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Evaluating...\n","  Batch    50  of     58.\n","\n","Training Loss: 0.578\n","Validation Loss: 0.568\n","\n"," Epoch 6 / 10\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Evaluating...\n","  Batch    50  of     58.\n","\n","Training Loss: 0.575\n","Validation Loss: 0.565\n","\n"," Epoch 7 / 10\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Evaluating...\n","  Batch    50  of     58.\n","\n","Training Loss: 0.565\n","Validation Loss: 0.578\n","\n"," Epoch 8 / 10\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Evaluating...\n","  Batch    50  of     58.\n","\n","Training Loss: 0.564\n","Validation Loss: 0.599\n","\n"," Epoch 9 / 10\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Evaluating...\n","  Batch    50  of     58.\n","\n","Training Loss: 0.561\n","Validation Loss: 0.575\n","\n"," Epoch 10 / 10\n","  Batch    50  of    135.\n","  Batch   100  of    135.\n","\n","Evaluating...\n","  Batch    50  of     58.\n","\n","Training Loss: 0.558\n","Validation Loss: 0.564\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rIbxQqhIvRoF","outputId":"c8ec1a3e-dfdb-46f4-f990-63e23d3c3cfb"},"source":["#load weights of best model\n","path = 'saved_weights.pt'\n","model.load_state_dict(torch.load(path))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"ePraoirtvVBT"},"source":["# get predictions for test data\n","with torch.no_grad():\n","  preds = model(test_seq.to(device), test_mask.to(device))\n","  preds = preds.detach().cpu().numpy()\n","  preds = np.argmax(preds, axis = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pmd2Zvc4FX3T","outputId":"c6679cd3-9f38-4518-9e2c-e75020580f40"},"source":["print(preds)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0 1 0 ... 0 1 1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qbLFvm3tFBfX"},"source":["import csv\n","import pandas as pd\n","\n","predresult =[]\n","\n","for item in preds:\n","  if item == 1:\n","    item = 'NOT_SARCASM'\n","  else:\n","    item = 'SARCASM'\n","  predresult.append(item)\n","\n","with open ('testdata.csv') as f:\n","  reader = csv.reader(f)\n","  column = [row[2] for row in reader]\n","\n","#create test answer csv document\n","with open ('testans.csv', 'w+', newline='', encoding='utf-8') as f:\n","    csv_writer = csv.writer(f)\n","\n","    for i in range(len(predresult)):\n","      data_row = [column[i+1],predresult[i]]\n","      csv_writer.writerow(data_row)\n","\n","#conver csv to txt\n","output = open ('answer.txt','w')\n","with open('testans.csv', encoding='utf-8') as f:\n","    for row in f:\n","      output.write(row)"],"execution_count":null,"outputs":[]}]}