{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Course_project_410_BERT_REMOVE_STOPWORDS_AND_UNNECESSARY_PUNCUATIONS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bzhao10/CourseProject/blob/main/Code/Course_project_410_BERT_REMOVE_STOPWORDS_AND_UNNECESSARY_PUNCUATIONS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtwexhukQ5Cn"
      },
      "source": [
        "!pip install jsonlines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAhreXcYUBTm"
      },
      "source": [
        "import os\n",
        "import jsonlines\n",
        "import csv\n",
        "import pandas as pd\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#The following method is used to create readable traindata in csv format\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "data = []\n",
        "\n",
        "#Convert 'SARCASM' label in train.jsonl into 0 and 1 \n",
        "with open(\"train.jsonl\",\"r+\",encoding='utf-8') as f:\n",
        "    for item in jsonlines.Reader(f):\n",
        "        if item['label'] == 'SARCASM':\n",
        "            item['label'] = 0\n",
        "        else:\n",
        "            item['label'] = 1\n",
        "\n",
        "        data.append(item)\n",
        "\n",
        "#Remove stopwords and unnecessary puncuations and parse the train.jsonl data into csv format\n",
        "with open('traindata.csv', 'w+', newline='', encoding='utf-8') as f:\n",
        "    csv_writer = csv.writer(f)\n",
        "    csv_head = ['context', 'response', 'label']\n",
        "    csv_writer.writerow(csv_head)\n",
        "\n",
        "    for i in range(len(data)):\n",
        "      data[i]['context'] =' '.join([word for word in str(data[i]['context']).split() if not word in stopwords.words('english')])\n",
        "      data[i]['response'] =' '.join([word for word in str(data[i]['response']).split() if not word in stopwords.words('english')])\n",
        " \n",
        "    for i in range(len(data)):\n",
        "        data_row = [(data[i]['context']).replace('@USER', '').replace('<URL>', '').replace('[', '').replace(']', '').replace(\"'\",  '').replace('\"', '').replace('.', '').replace('#', ''),\n",
        "                    (data[i]['response']).replace('@USER', '').replace('<URL>','').replace(\"'\",  '').replace('\"', '').replace('.', '').replace('#', '').replace('.', ''), data[i]['label']]\n",
        "        csv_writer.writerow(data_row)\n",
        "\n",
        "total = pd.read_csv(\"traindata.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jag_Pj7tQklb"
      },
      "source": [
        "#The following method is used to create readable testdata in csv format\n",
        "\n",
        "data = []\n",
        "\n",
        "with open('test.jsonl',\"r+\",encoding='utf-8') as f:\n",
        "    for item in jsonlines.Reader(f):\n",
        "        data.append(item)\n",
        "        \n",
        "#Remove stopwords and unnecessary puncuations and parse the train.jsonl data into csv format\n",
        "with open('testdata.csv', 'w+', newline='', encoding='utf-8') as f:\n",
        "    csv_writer = csv.writer(f)\n",
        "    csv_head = ['context', 'response', 'id']\n",
        "    csv_writer.writerow(csv_head)\n",
        "\n",
        "    for i in range(len(data)):\n",
        "      data[i]['context'] =' '.join([word for word in str(data[i]['context']).split() if not word in stopwords.words('english')])\n",
        "      data[i]['response'] =' '.join([word for word in str(data[i]['response']).split() if not word in stopwords.words('english')])\n",
        "    \n",
        "    for i in range(len(data)):\n",
        "        data_row = [(data[i]['context']).replace('@USER', '').replace('<URL>', '').replace('[', '').replace(']', '').replace(\"'\",  '').replace('\"', '').replace('.', '').replace('#', ''),\n",
        "                    (data[i]['response']).replace('@USER', '').replace('<URL>','').replace(\"'\",  '').replace('\"', '').replace('.', '').replace('#', '').replace('.', ''), data[i]['id']]\n",
        "        csv_writer.writerow(data_row)\n",
        "\n",
        "total = pd.read_csv(\"testdata.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZRSiDYNQLSr"
      },
      "source": [
        "!pip install transformers==3.5.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwruXiTXsORZ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "# specify CPU as our device, since our Code will exceed Google Colab GPU RAM limitation\n",
        "device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87T5BetZQYQv"
      },
      "source": [
        "# Create the train text entry in reponse + context format, \n",
        "# Based on our paper research, this combination is the best fit for the twitter text classification\n",
        "df = pd.read_csv(\"traindata.csv\")\n",
        "df['text'] = df['response']+df['context']\n",
        "df.head() # Check the first 5 entries in the traindata.csv file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xalsFDNyQp66"
      },
      "source": [
        "# Create the test text entry in reponse + context format, \n",
        "# Based on our paper research, this combination is the best fit for the twitter text classification\n",
        "td = pd.read_csv(\"testdata.csv\")\n",
        "td['text'] = td['response']+td['context'] \n",
        "td.head() # Check the first 5 entries in the testdata.csv file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XryjXIYhQ_gM"
      },
      "source": [
        "# Separate the traindata.csv into train and validation dataset\n",
        "train_text, val_text, train_labels, val_labels = train_test_split(df['text'], df['label'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "# Create test dataset to generate the answer.txt file from best model\n",
        "test_text = td['text']    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG9h0KEotC_r"
      },
      "source": [
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPnTpMt8tJLp"
      },
      "source": [
        "# Initialize the send_id\n",
        "text = [\"this is a bert model \", \"we will fine-tune a bert model\"]\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmGxHveVtJ4f",
        "outputId": "72ce1721-c7f8-43d3-da56-e7b7cd780cb1"
      },
      "source": [
        "print(sent_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 102, 0, 0, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "TGYSbQzUtLwS",
        "outputId": "b36a4202-bdd5-4ad2-d725-6b88d7cf52df"
      },
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd5cbda8be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMzKcnYxtOFP"
      },
      "source": [
        "#Initialize the max sequence length\n",
        "max_seq_len = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6mRLN3vtQXX",
        "outputId": "d6124064-45ec-4ff2-9b66-8e7e58d2f5a6"
      },
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD_IfCzotVxj"
      },
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaDhHt7NtZRZ"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size \n",
        "batch_size = 29\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc380qoYtfE5"
      },
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0nqk53itiDN"
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyVFwzk5tjXo"
      },
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGHNgzsDtqfP"
      },
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MHWVvVDttbi",
        "outputId": "c7523f92-7980-45e9-f353-aa2bd97ef820"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "\n",
        "print(class_wts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytxaBXCDtwk_"
      },
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlrAx2VLt0F5"
      },
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmW_LsLct4lQ"
      },
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      #elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqLI_oGJt58w",
        "outputId": "19a389cc-7610-4b90-82d5-0ee3a26213c3"
      },
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of    121.\n",
            "  Batch   100  of    121.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     52.\n",
            "\n",
            "Training Loss: 0.661\n",
            "Validation Loss: 0.603\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of    121.\n",
            "  Batch   100  of    121.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     52.\n",
            "\n",
            "Training Loss: 0.605\n",
            "Validation Loss: 0.603\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of    121.\n",
            "  Batch   100  of    121.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     52.\n",
            "\n",
            "Training Loss: 0.579\n",
            "Validation Loss: 0.606\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of    121.\n",
            "  Batch   100  of    121.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     52.\n",
            "\n",
            "Training Loss: 0.588\n",
            "Validation Loss: 0.571\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of    121.\n",
            "  Batch   100  of    121.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     52.\n",
            "\n",
            "Training Loss: 0.573\n",
            "Validation Loss: 0.590\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of    121.\n",
            "  Batch   100  of    121.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     52.\n",
            "\n",
            "Training Loss: 0.581\n",
            "Validation Loss: 0.567\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of    121.\n",
            "  Batch   100  of    121.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     52.\n",
            "\n",
            "Training Loss: 0.563\n",
            "Validation Loss: 0.591\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of    121.\n",
            "  Batch   100  of    121.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     52.\n",
            "\n",
            "Training Loss: 0.576\n",
            "Validation Loss: 0.569\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of    121.\n",
            "  Batch   100  of    121.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     52.\n",
            "\n",
            "Training Loss: 0.554\n",
            "Validation Loss: 0.575\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of    121.\n",
            "  Batch   100  of    121.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     52.\n",
            "\n",
            "Training Loss: 0.562\n",
            "Validation Loss: 0.568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIbxQqhIvRoF",
        "outputId": "a0eba7c7-cd09-48d4-8df4-cbcdc1193270"
      },
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePraoirtvVBT"
      },
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()\n",
        "  preds = np.argmax(preds, axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmd2Zvc4FX3T",
        "outputId": "62b0a442-f6a2-499d-c4f0-938ee004d38a"
      },
      "source": [
        "print(preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 ... 0 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbLFvm3tFBfX"
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "predresult =[]\n",
        "\n",
        "for item in preds:\n",
        "  if item == 1:\n",
        "    item = 'NOT_SARCASM'\n",
        "  else:\n",
        "    item = 'SARCASM'\n",
        "  predresult.append(item)\n",
        "\n",
        "with open ('testdata.csv') as f:\n",
        "  reader = csv.reader(f)\n",
        "  column = [row[2] for row in reader]\n",
        "\n",
        "#create test answer csv document\n",
        "with open ('testans.csv', 'w+', newline='', encoding='utf-8') as f:\n",
        "    csv_writer = csv.writer(f)\n",
        "\n",
        "    for i in range(len(predresult)):\n",
        "      data_row = [column[i+1],predresult[i]]\n",
        "      csv_writer.writerow(data_row)\n",
        "\n",
        "#conver csv to txt\n",
        "output = open ('answer.txt','w')\n",
        "with open('testans.csv', encoding='utf-8') as f:\n",
        "    for row in f:\n",
        "      output.write(row)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}